{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9df322-d19b-4fb6-81f6-0545992e70bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-28 11:19:52.768629: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-28 11:19:52.944818: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-28 11:19:52.944879: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-28 11:19:52.959581: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-28 11:19:52.998784: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-28 11:19:53.658201: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3498 images belonging to 5 classes.\n",
      "Found 997 images belonging to 5 classes.\n",
      "Found 505 images belonging to 5 classes.\n",
      "3498\n",
      "997\n",
      "{'dia_ret', 'AMD', 'glaucoma', 'cataract', 'normal'}\n",
      "Found 3498 images belonging to 5 classes.\n",
      "Found 997 images belonging to 5 classes.\n",
      "3498\n",
      "997\n",
      "{'dia_ret', 'AMD', 'glaucoma', 'cataract', 'normal'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_data_dir=r\"/home/admin1/Desktop/new data with aug/data-split/train\"\n",
    "val_data_dir=r\"/home/admin1/Desktop/new data with aug/data-split/val\"\n",
    "test_data_dir=r\"/home/admin1/Desktop/new data with aug/data-split/test\"\n",
    "\n",
    "\n",
    "\n",
    "train_datagen=ImageDataGenerator(#rescale=1./255,\n",
    "                                 horizontal_flip=True,\n",
    "                                 )\n",
    "\n",
    "val_datagen=ImageDataGenerator()#rescale=1./255)\n",
    "test_datagen=ImageDataGenerator()#rescale=1./255)\n",
    "\n",
    "img_width,img_height=224,224\n",
    "batch_size=16\n",
    "train_generator=train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                  target_size=(img_height,img_width),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  class_mode='categorical')\n",
    "\n",
    "val_generator=val_datagen.flow_from_directory(val_data_dir,\n",
    "                                              target_size=(img_height,img_width),\n",
    "                                              batch_size=batch_size,\n",
    "                                              class_mode='categorical')\n",
    "\n",
    "test_generator=test_datagen.flow_from_directory(test_data_dir,\n",
    "                                              target_size=(img_height,img_width),\n",
    "                                              batch_size=batch_size,\n",
    "                                              class_mode='categorical')\n",
    "\n",
    "train_class_names = set()\n",
    "num_train_samples=0\n",
    "for i in train_generator.filenames:\n",
    "    train_class_names.add(i.split('/')[0])\n",
    "    num_train_samples+=1\n",
    "print(num_train_samples)\n",
    "train_class_names\n",
    "\n",
    "val_class_names = set()\n",
    "num_val_samples=0\n",
    "for i in val_generator.filenames:\n",
    "    val_class_names.add(i.split('/')[0])\n",
    "    num_val_samples+=1\n",
    "print(num_val_samples)\n",
    "print(val_class_names)\n",
    "\n",
    "num_classes = len(val_class_names)\n",
    "num_classes\n",
    "\n",
    "\n",
    "train_datagen=ImageDataGenerator(#rescale=1./255,\n",
    "                                 rotation_range=45,\n",
    "                                 width_shift_range=0.3,\n",
    "                                 height_shift_range=0.3,\n",
    "                                 horizontal_flip=True,\n",
    "                                 fill_mode='nearest')\n",
    "\n",
    "val_datagen=ImageDataGenerator()#rescale=1./255)\n",
    "\n",
    "img_width,img_height=224,224\n",
    "batch_size=32\n",
    "train_generator=train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                  target_size=(img_height,img_width),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  class_mode='categorical')\n",
    "\n",
    "val_generator=val_datagen.flow_from_directory(val_data_dir,\n",
    "                                              target_size=(img_height,img_width),\n",
    "                                              batch_size=batch_size,\n",
    "                                              class_mode='categorical')\n",
    "\n",
    "train_class_names = set()\n",
    "num_train_samples=0\n",
    "for i in train_generator.filenames:\n",
    "    train_class_names.add(i.split('/')[0])\n",
    "    num_train_samples+=1\n",
    "print(num_train_samples)\n",
    "train_class_names\n",
    "\n",
    "val_class_names = set()\n",
    "num_val_samples=0\n",
    "for i in val_generator.filenames:\n",
    "    val_class_names.add(i.split('/')[0])\n",
    "    num_val_samples+=1\n",
    "print(num_val_samples)\n",
    "print(val_class_names)\n",
    "\n",
    "num_classes = len(val_class_names)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e48b7f5d-cacf-4382-99a0-407119b4ae07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-28 11:20:16.472475: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-28 11:20:16.548449: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.layers import GlobalAveragePooling2D,Conv2D, Flatten, BatchNormalization, Dense, Dropout\n",
    "from tensorflow.keras.layers import MaxPooling2D, GlobalAveragePooling2D\n",
    "base_model = InceptionV3(\n",
    "                    input_shape=(224, 224, 3),\n",
    "                    weights='imagenet',\n",
    "                    include_top=False)\n",
    "# Freeze the first 10 layers\n",
    "for layer in base_model.layers[:10]:\n",
    "    layer.trainable = False\n",
    "x = base_model.output\n",
    "x = Conv2D(128,(3,3),activation='relu')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "model1 = Model(inputs=base_model.inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bfb01d5-1a5f-4962-ab4f-5ed03652646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import tensorflow as tf\n",
    "checkpoint = ModelCheckpoint(\"augnewInceptionV3.h5\",\n",
    "                             monitor='val_loss',\n",
    "                             mode='min',\n",
    "                             save_best_only=True,verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0,\n",
    "                          patience=7,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True)\n",
    "\n",
    "callbacks=[checkpoint,earlystop]\n",
    "\n",
    "epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db406b56-4071-482f-9cfb-73124aa7bcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "109/109 [==============================] - ETA: 0s - loss: 1.0254 - accuracy: 0.5788 - auc: 0.8639\n",
      "Epoch 1: val_loss improved from inf to 2.33236, saving model to augnewInceptionV3.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin1/miniconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109/109 [==============================] - 171s 2s/step - loss: 1.0254 - accuracy: 0.5788 - auc: 0.8639 - val_loss: 2.3324 - val_accuracy: 0.5242 - val_auc: 0.7930\n",
      "Epoch 2/20\n",
      "109/109 [==============================] - ETA: 0s - loss: 0.7179 - accuracy: 0.7409 - auc: 0.9356\n",
      "Epoch 2: val_loss improved from 2.33236 to 0.79566, saving model to augnewInceptionV3.h5\n",
      "109/109 [==============================] - 165s 2s/step - loss: 0.7179 - accuracy: 0.7409 - auc: 0.9356 - val_loss: 0.7957 - val_accuracy: 0.8024 - val_auc: 0.9497\n",
      "Epoch 3/20\n",
      "109/109 [==============================] - ETA: 0s - loss: 0.5669 - accuracy: 0.8012 - auc: 0.9593\n",
      "Epoch 3: val_loss improved from 0.79566 to 0.58330, saving model to augnewInceptionV3.h5\n",
      "109/109 [==============================] - 166s 2s/step - loss: 0.5669 - accuracy: 0.8012 - auc: 0.9593 - val_loss: 0.5833 - val_accuracy: 0.8508 - val_auc: 0.9628\n",
      "Epoch 4/20\n",
      "109/109 [==============================] - ETA: 0s - loss: 0.4929 - accuracy: 0.8226 - auc: 0.9689\n",
      "Epoch 4: val_loss did not improve from 0.58330\n",
      "109/109 [==============================] - 162s 1s/step - loss: 0.4929 - accuracy: 0.8226 - auc: 0.9689 - val_loss: 0.8568 - val_accuracy: 0.7873 - val_auc: 0.9422\n",
      "Epoch 5/20\n",
      "109/109 [==============================] - ETA: 0s - loss: 0.4220 - accuracy: 0.8609 - auc: 0.9765\n",
      "Epoch 5: val_loss improved from 0.58330 to 0.43952, saving model to augnewInceptionV3.h5\n",
      "109/109 [==============================] - 164s 2s/step - loss: 0.4220 - accuracy: 0.8609 - auc: 0.9765 - val_loss: 0.4395 - val_accuracy: 0.8891 - val_auc: 0.9770\n",
      "Epoch 6/20\n",
      "109/109 [==============================] - ETA: 0s - loss: 0.3775 - accuracy: 0.8693 - auc: 0.9802\n",
      "Epoch 6: val_loss did not improve from 0.43952\n",
      "109/109 [==============================] - 163s 1s/step - loss: 0.3775 - accuracy: 0.8693 - auc: 0.9802 - val_loss: 0.6403 - val_accuracy: 0.8599 - val_auc: 0.9642\n",
      "Epoch 7/20\n",
      "109/109 [==============================] - ETA: 0s - loss: 0.3197 - accuracy: 0.8869 - auc: 0.9857\n",
      "Epoch 7: val_loss improved from 0.43952 to 0.37722, saving model to augnewInceptionV3.h5\n",
      "109/109 [==============================] - 162s 1s/step - loss: 0.3197 - accuracy: 0.8869 - auc: 0.9857 - val_loss: 0.3772 - val_accuracy: 0.9062 - val_auc: 0.9808\n",
      "Epoch 8/20\n",
      "109/109 [==============================] - ETA: 0s - loss: 0.3289 - accuracy: 0.8941 - auc: 0.9850\n",
      "Epoch 8: val_loss did not improve from 0.37722\n",
      "109/109 [==============================] - 162s 1s/step - loss: 0.3289 - accuracy: 0.8941 - auc: 0.9850 - val_loss: 0.4605 - val_accuracy: 0.8790 - val_auc: 0.9781\n",
      "Epoch 9/20\n",
      "109/109 [==============================] - ETA: 0s - loss: 0.3053 - accuracy: 0.8973 - auc: 0.9862\n",
      "Epoch 9: val_loss improved from 0.37722 to 0.26244, saving model to augnewInceptionV3.h5\n",
      "109/109 [==============================] - 162s 1s/step - loss: 0.3053 - accuracy: 0.8973 - auc: 0.9862 - val_loss: 0.2624 - val_accuracy: 0.9183 - val_auc: 0.9911\n",
      "Epoch 10/20\n",
      "109/109 [==============================] - ETA: 0s - loss: 0.2595 - accuracy: 0.9111 - auc: 0.9898\n",
      "Epoch 10: val_loss did not improve from 0.26244\n",
      "109/109 [==============================] - 164s 2s/step - loss: 0.2595 - accuracy: 0.9111 - auc: 0.9898 - val_loss: 0.7796 - val_accuracy: 0.8488 - val_auc: 0.9578\n",
      "Epoch 11/20\n",
      "109/109 [==============================] - ETA: 0s - loss: 0.2824 - accuracy: 0.9117 - auc: 0.9874\n",
      "Epoch 11: val_loss did not improve from 0.26244\n",
      "109/109 [==============================] - 164s 2s/step - loss: 0.2824 - accuracy: 0.9117 - auc: 0.9874 - val_loss: 0.5327 - val_accuracy: 0.8750 - val_auc: 0.9710\n",
      "Epoch 12/20\n",
      "109/109 [==============================] - ETA: 0s - loss: 0.2283 - accuracy: 0.9218 - auc: 0.9914\n",
      "Epoch 12: val_loss improved from 0.26244 to 0.20957, saving model to augnewInceptionV3.h5\n",
      "109/109 [==============================] - 163s 1s/step - loss: 0.2283 - accuracy: 0.9218 - auc: 0.9914 - val_loss: 0.2096 - val_accuracy: 0.9355 - val_auc: 0.9933\n",
      "Epoch 13/20\n",
      "109/109 [==============================] - ETA: 0s - loss: 0.2161 - accuracy: 0.9276 - auc: 0.9920\n",
      "Epoch 13: val_loss did not improve from 0.20957\n",
      "109/109 [==============================] - 162s 1s/step - loss: 0.2161 - accuracy: 0.9276 - auc: 0.9920 - val_loss: 0.2830 - val_accuracy: 0.9355 - val_auc: 0.9885\n",
      "Epoch 14/20\n",
      "109/109 [==============================] - ETA: 0s - loss: 0.2274 - accuracy: 0.9293 - auc: 0.9925\n",
      "Epoch 14: val_loss did not improve from 0.20957\n",
      "109/109 [==============================] - 161s 1s/step - loss: 0.2274 - accuracy: 0.9293 - auc: 0.9925 - val_loss: 0.5819 - val_accuracy: 0.8800 - val_auc: 0.9718\n",
      "Epoch 15/20\n",
      "109/109 [==============================] - ETA: 0s - loss: 0.1985 - accuracy: 0.9371 - auc: 0.9935\n",
      "Epoch 15: val_loss did not improve from 0.20957\n",
      "109/109 [==============================] - 163s 1s/step - loss: 0.1985 - accuracy: 0.9371 - auc: 0.9935 - val_loss: 0.6786 - val_accuracy: 0.8891 - val_auc: 0.9625\n",
      "Epoch 16/20\n",
      "109/109 [==============================] - ETA: 0s - loss: 0.1664 - accuracy: 0.9458 - auc: 0.9949\n",
      "Epoch 16: val_loss did not improve from 0.20957\n",
      "109/109 [==============================] - 162s 1s/step - loss: 0.1664 - accuracy: 0.9458 - auc: 0.9949 - val_loss: 0.7518 - val_accuracy: 0.8992 - val_auc: 0.9630\n",
      "Epoch 17/20\n",
      "109/109 [==============================] - ETA: 0s - loss: 0.1698 - accuracy: 0.9460 - auc: 0.9943\n",
      "Epoch 17: val_loss did not improve from 0.20957\n",
      "109/109 [==============================] - 162s 1s/step - loss: 0.1698 - accuracy: 0.9460 - auc: 0.9943 - val_loss: 0.3119 - val_accuracy: 0.9345 - val_auc: 0.9859\n",
      "Epoch 18/20\n",
      "109/109 [==============================] - ETA: 0s - loss: 0.1564 - accuracy: 0.9458 - auc: 0.9958\n",
      "Epoch 18: val_loss did not improve from 0.20957\n",
      "109/109 [==============================] - 161s 1s/step - loss: 0.1564 - accuracy: 0.9458 - auc: 0.9958 - val_loss: 0.4914 - val_accuracy: 0.9113 - val_auc: 0.9766\n",
      "Epoch 19/20\n",
      "109/109 [==============================] - ETA: 0s - loss: 0.1954 - accuracy: 0.9426 - auc: 0.9928\n",
      "Epoch 19: val_loss did not improve from 0.20957\n",
      "Restoring model weights from the end of the best epoch: 12.\n",
      "109/109 [==============================] - 165s 2s/step - loss: 0.1954 - accuracy: 0.9426 - auc: 0.9928 - val_loss: 0.5025 - val_accuracy: 0.9103 - val_auc: 0.9776\n",
      "Epoch 19: early stopping\n"
     ]
    }
   ],
   "source": [
    "model1.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=RMSprop(learning_rate=0.0001),\n",
    "                   metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "history1 = model1.fit(train_generator,\n",
    "                         steps_per_epoch=num_train_samples//batch_size,\n",
    "                         epochs=epochs,\n",
    "                         callbacks=callbacks,\n",
    "                         validation_data=val_generator,\n",
    "                         validation_steps=num_val_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "672bd1cf-c7e2-40cc-9e0a-a5a2173cb5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def predict_class(image_path, model, train_generator):\n",
    "    # Load the image\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "\n",
    "    # Convert the image to a numpy array\n",
    "    img_array = img_to_array(img)\n",
    "\n",
    "    # Expand the dimensions of the numpy array\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    # Predict the class of the image\n",
    "    class_prediction = model.predict(img_array)\n",
    "\n",
    "    # Get the predicted class index\n",
    "    class_index = np.argmax(class_prediction)\n",
    "\n",
    "    # Get the class indices and names\n",
    "    class_indices = train_generator.class_indices\n",
    "    class_names = dict((v, k) for k, v in class_indices.items())\n",
    "\n",
    "    # Get the predicted class name\n",
    "    class_name = class_names[class_index]\n",
    "\n",
    "    # Return the predicted class name\n",
    "    return class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9a65295-fcab-484f-9616-2633ec7ed577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the model from the .h5 file\n",
    "loaded_model = tf.keras.models.load_model('augnewInceptionV3.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b7d40e5-a1c0-4430-b2a8-71623d039157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 641ms/step\n",
      "AMD\n"
     ]
    }
   ],
   "source": [
    "user_input = \"/home/admin1/Desktop/BE project/final dataset/AMD/AMD_107.png\"\n",
    "path = user_input\n",
    "class_index = predict_class(path, loaded_model, train_generator)\n",
    "print(class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb8fe11a-13b0-4acf-b259-688b97c0ee61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "cataract\n"
     ]
    }
   ],
   "source": [
    "user_input = \"/home/admin1/Desktop/BE project/final dataset/cataract/cataract_233.jpg\"\n",
    "path = user_input\n",
    "class_index = predict_class(path, loaded_model, train_generator)\n",
    "print(class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9eebd8ff-054c-4eeb-86d3-d2ce5ff6ce5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "dia_ret\n"
     ]
    }
   ],
   "source": [
    "user_input = \"/home/admin1/Desktop/BE project/final dataset/dia_ret/dia_ret_232.jpg\"\n",
    "path = user_input\n",
    "class_index = predict_class(path, loaded_model, train_generator)\n",
    "print(class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72adf0ec-1d08-4cd1-918b-0e5e3143460a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n",
      "glaucoma\n"
     ]
    }
   ],
   "source": [
    "user_input = \"/home/admin1/Desktop/BE project/final dataset/glaucoma/glaucoma_199.JPG\"\n",
    "path = user_input\n",
    "class_index = predict_class(path, loaded_model, train_generator)\n",
    "print(class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2d85703-18f6-4c62-a0a0-3d920d76775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n",
      "normal\n"
     ]
    }
   ],
   "source": [
    "user_input = \"/home/admin1/Desktop/BE project/final dataset/normal/normal_204.png\"\n",
    "path = user_input\n",
    "class_index = predict_class(path, loaded_model, train_generator)\n",
    "print(class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69da3dcd-0cef-41bf-8185-4fa9767ffee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d02e740-37ed-4993-a1ef-de0265ee0a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 505 images belonging to 5 classes.\n",
      "16/16 [==============================] - 18s 1s/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMD       0.98      0.92      0.95       102\n",
      "    cataract       1.00      0.95      0.97       102\n",
      "     dia_ret       0.88      0.97      0.92       110\n",
      "    glaucoma       0.94      0.85      0.89       102\n",
      "      normal       0.84      0.91      0.87        89\n",
      "\n",
      "    accuracy                           0.92       505\n",
      "   macro avg       0.93      0.92      0.92       505\n",
      "weighted avg       0.93      0.92      0.92       505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "# Assuming you have test data in a separate directory\n",
    "test_data_dir = \"/home/admin1/Desktop/new data with aug/data-split/test\"\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_data_dir,\n",
    "                                                  target_size=(img_height, img_width),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False,\n",
    "                                                  class_mode='categorical')\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model1.predict(test_generator, steps=len(test_generator), verbose=1)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "y_pred_classes = [np.argmax(pred) for pred in y_pred]\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "# Get class labels\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_classes, y_pred_classes, target_names=class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4be9efa-beab-4917-952f-dd4cefeef759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
