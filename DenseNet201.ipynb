{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2876ed46-273b-4785-8162-2ec9a354efb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-28 13:28:59.551992: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-28 13:28:59.687886: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-28 13:28:59.687957: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-28 13:28:59.708875: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-28 13:28:59.764813: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-28 13:29:00.428205: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3498 images belonging to 5 classes.\n",
      "Found 997 images belonging to 5 classes.\n",
      "Found 505 images belonging to 5 classes.\n",
      "3498\n",
      "997\n",
      "{'normal', 'cataract', 'dia_ret', 'glaucoma', 'AMD'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_data_dir=r\"/home/admin1/Desktop/new data with aug/data-split/train\"\n",
    "val_data_dir=r\"/home/admin1/Desktop/new data with aug/data-split/val\"\n",
    "test_data_dir=r\"/home/admin1/Desktop/new data with aug/data-split/test\"\n",
    "\n",
    "\n",
    "\n",
    "train_datagen=ImageDataGenerator(#rescale=1./255,\n",
    "                                 horizontal_flip=True,\n",
    "                                 )\n",
    "\n",
    "val_datagen=ImageDataGenerator()#rescale=1./255)\n",
    "test_datagen=ImageDataGenerator()#rescale=1./255)\n",
    "\n",
    "img_width,img_height=224,224\n",
    "batch_size=16\n",
    "train_generator=train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                  target_size=(img_height,img_width),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  class_mode='categorical')\n",
    "\n",
    "val_generator=val_datagen.flow_from_directory(val_data_dir,\n",
    "                                              target_size=(img_height,img_width),\n",
    "                                              batch_size=batch_size,\n",
    "                                              class_mode='categorical')\n",
    "\n",
    "test_generator=test_datagen.flow_from_directory(test_data_dir,\n",
    "                                              target_size=(img_height,img_width),\n",
    "                                              batch_size=batch_size,\n",
    "                                              class_mode='categorical')\n",
    "\n",
    "train_class_names = set()\n",
    "num_train_samples=0\n",
    "for i in train_generator.filenames:\n",
    "    train_class_names.add(i.split('/')[0])\n",
    "    num_train_samples+=1\n",
    "print(num_train_samples)\n",
    "train_class_names\n",
    "\n",
    "val_class_names = set()\n",
    "num_val_samples=0\n",
    "for i in val_generator.filenames:\n",
    "    val_class_names.add(i.split('/')[0])\n",
    "    num_val_samples+=1\n",
    "print(num_val_samples)\n",
    "print(val_class_names)\n",
    "\n",
    "num_classes = len(val_class_names)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bce9008a-2c22-4dd3-8268-7b2405ae2568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-28 13:29:03.561180: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-28 13:29:03.646672: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import DenseNet201\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D,Conv2D, Flatten, BatchNormalization, Dense, Dropout\n",
    "from tensorflow.keras.layers import MaxPooling2D, GlobalAveragePooling2D\n",
    "base_model = DenseNet201(\n",
    "                    input_shape=(224, 224, 3),\n",
    "                    weights='imagenet',\n",
    "                    include_top=False)\n",
    "# Freeze the first 10 layers\n",
    "for layer in base_model.layers[:10]:\n",
    "    layer.trainable = False\n",
    "x = base_model.output\n",
    "x = Conv2D(128,(3,3),activation='relu')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "model1 = Model(inputs=base_model.inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c4dc16b-a1c1-4f7c-9ecd-55c6108a354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import tensorflow as tf\n",
    "checkpoint = ModelCheckpoint(\"aug_DenseNet201.h5\",\n",
    "                             monitor='val_loss',\n",
    "                             mode='min',\n",
    "                             save_best_only=True,verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0,\n",
    "                          patience=7,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True)\n",
    "\n",
    "callbacks=[checkpoint,earlystop]\n",
    "\n",
    "epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcd9fb55-1db1-4cd6-a18f-52b158313cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.4621 - accuracy: 0.8326 - auc: 0.9724\n",
      "Epoch 1: val_loss improved from inf to 0.31720, saving model to aug_DenseNet201.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin1/miniconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218/218 [==============================] - 318s 1s/step - loss: 0.4621 - accuracy: 0.8326 - auc: 0.9724 - val_loss: 0.3172 - val_accuracy: 0.9183 - val_auc: 0.9867\n",
      "Epoch 2/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.2148 - accuracy: 0.9319 - auc: 0.9916\n",
      "Epoch 2: val_loss improved from 0.31720 to 0.13537, saving model to aug_DenseNet201.h5\n",
      "218/218 [==============================] - 297s 1s/step - loss: 0.2148 - accuracy: 0.9319 - auc: 0.9916 - val_loss: 0.1354 - val_accuracy: 0.9688 - val_auc: 0.9932\n",
      "Epoch 3/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.1510 - accuracy: 0.9520 - auc: 0.9947\n",
      "Epoch 3: val_loss improved from 0.13537 to 0.07633, saving model to aug_DenseNet201.h5\n",
      "218/218 [==============================] - 298s 1s/step - loss: 0.1510 - accuracy: 0.9520 - auc: 0.9947 - val_loss: 0.0763 - val_accuracy: 0.9849 - val_auc: 0.9970\n",
      "Epoch 4/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.0842 - accuracy: 0.9753 - auc: 0.9981\n",
      "Epoch 4: val_loss did not improve from 0.07633\n",
      "218/218 [==============================] - 297s 1s/step - loss: 0.0842 - accuracy: 0.9753 - auc: 0.9981 - val_loss: 0.1237 - val_accuracy: 0.9768 - val_auc: 0.9947\n",
      "Epoch 5/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.1019 - accuracy: 0.9773 - auc: 0.9964\n",
      "Epoch 5: val_loss did not improve from 0.07633\n",
      "218/218 [==============================] - 296s 1s/step - loss: 0.1019 - accuracy: 0.9773 - auc: 0.9964 - val_loss: 0.1293 - val_accuracy: 0.9819 - val_auc: 0.9931\n",
      "Epoch 6/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9779 - auc: 0.9982\n",
      "Epoch 6: val_loss did not improve from 0.07633\n",
      "218/218 [==============================] - 296s 1s/step - loss: 0.0681 - accuracy: 0.9779 - auc: 0.9982 - val_loss: 0.2295 - val_accuracy: 0.9577 - val_auc: 0.9882\n",
      "Epoch 7/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9845 - auc: 0.9980\n",
      "Epoch 7: val_loss did not improve from 0.07633\n",
      "218/218 [==============================] - 296s 1s/step - loss: 0.0709 - accuracy: 0.9845 - auc: 0.9980 - val_loss: 0.1442 - val_accuracy: 0.9778 - val_auc: 0.9930\n",
      "Epoch 8/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.0603 - accuracy: 0.9865 - auc: 0.9975\n",
      "Epoch 8: val_loss did not improve from 0.07633\n",
      "218/218 [==============================] - 296s 1s/step - loss: 0.0603 - accuracy: 0.9865 - auc: 0.9975 - val_loss: 0.1057 - val_accuracy: 0.9808 - val_auc: 0.9951\n",
      "Epoch 9/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.0444 - accuracy: 0.9888 - auc: 0.9985\n",
      "Epoch 9: val_loss did not improve from 0.07633\n",
      "218/218 [==============================] - 295s 1s/step - loss: 0.0444 - accuracy: 0.9888 - auc: 0.9985 - val_loss: 0.1068 - val_accuracy: 0.9808 - val_auc: 0.9952\n",
      "Epoch 10/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.0614 - accuracy: 0.9871 - auc: 0.9981\n",
      "Epoch 10: val_loss did not improve from 0.07633\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "218/218 [==============================] - 295s 1s/step - loss: 0.0614 - accuracy: 0.9871 - auc: 0.9981 - val_loss: 0.0987 - val_accuracy: 0.9778 - val_auc: 0.9960\n",
      "Epoch 10: early stopping\n"
     ]
    }
   ],
   "source": [
    "model1.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=RMSprop(learning_rate=0.0001),\n",
    "                   metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "history1 = model1.fit(train_generator,\n",
    "                         steps_per_epoch=num_train_samples//batch_size,\n",
    "                         epochs=epochs,\n",
    "                         callbacks=callbacks,\n",
    "                         validation_data=val_generator,\n",
    "                         validation_steps=num_val_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2662a359-36d3-43a2-a151-b74f84de22d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def predict_class(image_path, model, train_generator):\n",
    "    # Load the image\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "\n",
    "    # Convert the image to a numpy array\n",
    "    img_array = img_to_array(img)\n",
    "\n",
    "    # Expand the dimensions of the numpy array\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    # Predict the class of the image\n",
    "    class_prediction = model.predict(img_array)\n",
    "\n",
    "    # Get the predicted class index\n",
    "    class_index = np.argmax(class_prediction)\n",
    "\n",
    "    # Get the class indices and names\n",
    "    class_indices = train_generator.class_indices\n",
    "    class_names = dict((v, k) for k, v in class_indices.items())\n",
    "\n",
    "    # Get the predicted class name\n",
    "    class_name = class_names[class_index]\n",
    "\n",
    "    # Return the predicted class name\n",
    "    return class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da3e46d5-e4dc-4316-8255-b6113ca93f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the model from the .h5 file\n",
    "loaded_model = tf.keras.models.load_model('aug_DenseNet201.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37db7c29-83e8-4f5b-ac4c-2d5f3f1f1ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "AMD\n"
     ]
    }
   ],
   "source": [
    "user_input = \"/home/admin1/Desktop/BE project/final dataset/AMD/AMD_107.png\"\n",
    "path = user_input\n",
    "class_index = predict_class(path, loaded_model, train_generator)\n",
    "print(class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23801fe3-e474-4c8f-a24d-f5e61f874fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 110ms/step\n",
      "cataract\n"
     ]
    }
   ],
   "source": [
    "user_input = \"/home/admin1/Desktop/BE project/final dataset/cataract/cataract_111.jpg\"\n",
    "path = user_input\n",
    "class_index = predict_class(path, loaded_model, train_generator)\n",
    "print(class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b3861f5-66c5-454c-a2c4-0592c8e84c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 445ms/step\n",
      "dia_ret\n"
     ]
    }
   ],
   "source": [
    "user_input = \"/home/admin1/Desktop/BE project/final dataset/dia_ret/dia_ret_261.jpg\"\n",
    "path = user_input\n",
    "class_index = predict_class(path, loaded_model, train_generator)\n",
    "print(class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13bd8a0d-306e-47fa-be46-dcd924ffd196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 302ms/step\n",
      "glaucoma\n"
     ]
    }
   ],
   "source": [
    "user_input = \"/home/admin1/Desktop/BE project/final dataset/glaucoma/glaucoma_277.jpg\"\n",
    "path = user_input\n",
    "class_index = predict_class(path, loaded_model, train_generator)\n",
    "print(class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d42ad9e6-af14-4d3b-b8c7-ec008fa9935d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 355ms/step\n",
      "normal\n"
     ]
    }
   ],
   "source": [
    "user_input = \"/home/admin1/Desktop/BE project/final dataset/normal/normal_62.jpeg\"\n",
    "path = user_input\n",
    "class_index = predict_class(path, loaded_model, train_generator)\n",
    "print(class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2433a2b-2db7-478a-baad-61bec04ba93a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0149c949-a3d3-4ee5-89da-9d22e3361fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 505 images belonging to 5 classes.\n",
      "32/32 [==============================] - 29s 746ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMD       0.99      0.99      0.99       102\n",
      "    cataract       1.00      1.00      1.00       102\n",
      "     dia_ret       1.00      0.99      1.00       110\n",
      "    glaucoma       0.95      0.99      0.97       102\n",
      "      normal       0.98      0.94      0.96        89\n",
      "\n",
      "    accuracy                           0.98       505\n",
      "   macro avg       0.98      0.98      0.98       505\n",
      "weighted avg       0.98      0.98      0.98       505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "# Assuming you have test data in a separate directory\n",
    "test_data_dir = \"/home/admin1/Desktop/new data with aug/data-split/test\"\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_data_dir,\n",
    "                                                  target_size=(img_height, img_width),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False,\n",
    "                                                  class_mode='categorical')\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model1.predict(test_generator, steps=len(test_generator), verbose=1)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "y_pred_classes = [np.argmax(pred) for pred in y_pred]\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "# Get class labels\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_classes, y_pred_classes, target_names=class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d504f7-8380-42a4-a47e-cb7c3cab6b52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
