{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e05a28de-e5c3-4266-8627-ed3727bbba8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 17:22:40.126795: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-29 17:22:40.162464: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-29 17:22:40.162491: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-29 17:22:40.163373: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-29 17:22:40.169124: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-29 17:22:40.855057: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_data_dir=\"/home/bharatforge/Desktop/BE aug new data/data-split/train\"\n",
    "val_data_dir=\"/home/bharatforge/Desktop/BE aug new data/data-split/val\"\n",
    "\n",
    "\n",
    "train_datagen=ImageDataGenerator(#rescale=1./255,\n",
    "                                 rotation_range=45,\n",
    "                                 width_shift_range=0.3,\n",
    "                                 height_shift_range=0.3,\n",
    "                                 horizontal_flip=True,\n",
    "                                 fill_mode='nearest')\n",
    "\n",
    "val_datagen=ImageDataGenerator()#rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1cd9743-1fba-432a-a51c-27a3014704c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3498 images belonging to 5 classes.\n",
      "Found 997 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "img_width,img_height=224,224\n",
    "batch_size=16\n",
    "train_generator=train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                  target_size=(img_height,img_width),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  class_mode='categorical')\n",
    "\n",
    "val_generator=val_datagen.flow_from_directory(val_data_dir,\n",
    "                                              target_size=(img_height,img_width),\n",
    "                                              batch_size=batch_size,\n",
    "                                              class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3150ac11-6b1c-4961-9203-b5665500f083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AMD', 'cataract', 'dia_ret', 'glaucoma', 'normal'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_class_names = set()\n",
    "num_train_samples=0\n",
    "for i in train_generator.filenames:\n",
    "    train_class_names.add(i.split('/')[0])\n",
    "    num_train_samples+=1\n",
    "print(num_train_samples)\n",
    "train_class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daa1feb7-25f4-445a-8d9c-5239c2f3a784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "997\n",
      "{'AMD', 'normal', 'dia_ret', 'cataract', 'glaucoma'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_class_names = set()\n",
    "num_val_samples=0\n",
    "for i in val_generator.filenames:\n",
    "    val_class_names.add(i.split('/')[0])\n",
    "    num_val_samples+=1\n",
    "print(num_val_samples)\n",
    "print(val_class_names)\n",
    "\n",
    "num_classes = len(val_class_names)\n",
    "num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c005d095-7f5b-4d3e-b1a7-b97b96037e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8bc890d-9c7e-4751-9106-b89fcc867d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 17:22:42.083130: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-29 17:22:42.110092: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-29 17:22:42.110262: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-29 17:22:42.111301: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-29 17:22:42.111430: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-29 17:22:42.111511: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-29 17:22:42.171524: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-29 17:22:42.171659: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-29 17:22:42.171752: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-29 17:22:42.171820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14392 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 17:24:09.824629: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape instacking_model/name_of_model_1/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2024-02-29 17:24:18.207149: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-02-29 17:24:18.677117: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-02-29 17:24:18.743766: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-02-29 17:24:23.274263: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f150d1bc140 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-02-29 17:24:23.274289: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA RTX A4000, Compute Capability 8.6\n",
      "2024-02-29 17:24:23.278942: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1709207663.338707   80425 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218/218 [==============================] - ETA: 0s - loss: 1.3423 - accuracy: 0.6554 - auc: 0.8813\n",
      "Epoch 1: val_loss improved from inf to 7.64654, saving model to AUG_ensemble.tf\n",
      "INFO:tensorflow:Assets written to: AUG_ensemble.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: AUG_ensemble.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218/218 [==============================] - 574s 2s/step - loss: 1.3423 - accuracy: 0.6554 - auc: 0.8813 - val_loss: 7.6465 - val_accuracy: 0.4032 - val_auc: 0.6698\n",
      "Epoch 2/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.8127 - accuracy: 0.7556 - auc: 0.9390\n",
      "Epoch 2: val_loss improved from 7.64654 to 0.59120, saving model to AUG_ensemble.tf\n",
      "INFO:tensorflow:Assets written to: AUG_ensemble.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: AUG_ensemble.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218/218 [==============================] - 417s 2s/step - loss: 0.8127 - accuracy: 0.7556 - auc: 0.9390 - val_loss: 0.5912 - val_accuracy: 0.8427 - val_auc: 0.9698\n",
      "Epoch 3/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.6555 - accuracy: 0.8024 - auc: 0.9578\n",
      "Epoch 3: val_loss did not improve from 0.59120\n",
      "218/218 [==============================] - 272s 1s/step - loss: 0.6555 - accuracy: 0.8024 - auc: 0.9578 - val_loss: 0.6580 - val_accuracy: 0.8216 - val_auc: 0.9651\n",
      "Epoch 4/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.6166 - accuracy: 0.8331 - auc: 0.9622\n",
      "Epoch 4: val_loss improved from 0.59120 to 0.46156, saving model to AUG_ensemble.tf\n",
      "INFO:tensorflow:Assets written to: AUG_ensemble.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: AUG_ensemble.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218/218 [==============================] - 421s 2s/step - loss: 0.6166 - accuracy: 0.8331 - auc: 0.9622 - val_loss: 0.4616 - val_accuracy: 0.8649 - val_auc: 0.9784\n",
      "Epoch 5/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.5661 - accuracy: 0.8501 - auc: 0.9678\n",
      "Epoch 5: val_loss did not improve from 0.46156\n",
      "218/218 [==============================] - 273s 1s/step - loss: 0.5661 - accuracy: 0.8501 - auc: 0.9678 - val_loss: 0.5765 - val_accuracy: 0.8216 - val_auc: 0.9729\n",
      "Epoch 6/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.5276 - accuracy: 0.8578 - auc: 0.9726\n",
      "Epoch 6: val_loss did not improve from 0.46156\n",
      "218/218 [==============================] - 272s 1s/step - loss: 0.5276 - accuracy: 0.8578 - auc: 0.9726 - val_loss: 1.1092 - val_accuracy: 0.7984 - val_auc: 0.9379\n",
      "Epoch 7/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.4582 - accuracy: 0.8756 - auc: 0.9783\n",
      "Epoch 7: val_loss did not improve from 0.46156\n",
      "218/218 [==============================] - 272s 1s/step - loss: 0.4582 - accuracy: 0.8756 - auc: 0.9783 - val_loss: 0.5268 - val_accuracy: 0.8861 - val_auc: 0.9816\n",
      "Epoch 8/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.4676 - accuracy: 0.8874 - auc: 0.9781\n",
      "Epoch 8: val_loss did not improve from 0.46156\n",
      "218/218 [==============================] - 277s 1s/step - loss: 0.4676 - accuracy: 0.8874 - auc: 0.9781 - val_loss: 0.7598 - val_accuracy: 0.8327 - val_auc: 0.9592\n",
      "Epoch 9/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.4149 - accuracy: 0.8952 - auc: 0.9828\n",
      "Epoch 9: val_loss improved from 0.46156 to 0.45484, saving model to AUG_ensemble.tf\n",
      "INFO:tensorflow:Assets written to: AUG_ensemble.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: AUG_ensemble.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218/218 [==============================] - 413s 2s/step - loss: 0.4149 - accuracy: 0.8952 - auc: 0.9828 - val_loss: 0.4548 - val_accuracy: 0.8911 - val_auc: 0.9813\n",
      "Epoch 10/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.4262 - accuracy: 0.8932 - auc: 0.9813\n",
      "Epoch 10: val_loss improved from 0.45484 to 0.42216, saving model to AUG_ensemble.tf\n",
      "INFO:tensorflow:Assets written to: AUG_ensemble.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: AUG_ensemble.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218/218 [==============================] - 417s 2s/step - loss: 0.4262 - accuracy: 0.8932 - auc: 0.9813 - val_loss: 0.4222 - val_accuracy: 0.9012 - val_auc: 0.9821\n",
      "Epoch 11/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.4111 - accuracy: 0.8972 - auc: 0.9828\n",
      "Epoch 11: val_loss did not improve from 0.42216\n",
      "218/218 [==============================] - 276s 1s/step - loss: 0.4111 - accuracy: 0.8972 - auc: 0.9828 - val_loss: 0.4462 - val_accuracy: 0.9103 - val_auc: 0.9795\n",
      "Epoch 12/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.3812 - accuracy: 0.9078 - auc: 0.9841\n",
      "Epoch 12: val_loss improved from 0.42216 to 0.26687, saving model to AUG_ensemble.tf\n",
      "INFO:tensorflow:Assets written to: AUG_ensemble.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: AUG_ensemble.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218/218 [==============================] - 416s 2s/step - loss: 0.3812 - accuracy: 0.9078 - auc: 0.9841 - val_loss: 0.2669 - val_accuracy: 0.9304 - val_auc: 0.9925\n",
      "Epoch 13/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.3695 - accuracy: 0.9072 - auc: 0.9856\n",
      "Epoch 13: val_loss did not improve from 0.26687\n",
      "218/218 [==============================] - 272s 1s/step - loss: 0.3695 - accuracy: 0.9072 - auc: 0.9856 - val_loss: 0.3214 - val_accuracy: 0.9073 - val_auc: 0.9906\n",
      "Epoch 14/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.3278 - accuracy: 0.9187 - auc: 0.9884\n",
      "Epoch 14: val_loss did not improve from 0.26687\n",
      "218/218 [==============================] - 271s 1s/step - loss: 0.3278 - accuracy: 0.9187 - auc: 0.9884 - val_loss: 0.6763 - val_accuracy: 0.8740 - val_auc: 0.9640\n",
      "Epoch 15/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.3802 - accuracy: 0.9087 - auc: 0.9846\n",
      "Epoch 15: val_loss did not improve from 0.26687\n",
      "218/218 [==============================] - 271s 1s/step - loss: 0.3802 - accuracy: 0.9087 - auc: 0.9846 - val_loss: 0.5315 - val_accuracy: 0.8901 - val_auc: 0.9764\n",
      "Epoch 16/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.3303 - accuracy: 0.9233 - auc: 0.9880\n",
      "Epoch 16: val_loss did not improve from 0.26687\n",
      "218/218 [==============================] - 271s 1s/step - loss: 0.3303 - accuracy: 0.9233 - auc: 0.9880 - val_loss: 0.6343 - val_accuracy: 0.8730 - val_auc: 0.9728\n",
      "Epoch 17/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.3426 - accuracy: 0.9161 - auc: 0.9874\n",
      "Epoch 17: val_loss improved from 0.26687 to 0.26507, saving model to AUG_ensemble.tf\n",
      "INFO:tensorflow:Assets written to: AUG_ensemble.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: AUG_ensemble.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218/218 [==============================] - 419s 2s/step - loss: 0.3426 - accuracy: 0.9161 - auc: 0.9874 - val_loss: 0.2651 - val_accuracy: 0.9284 - val_auc: 0.9939\n",
      "Epoch 18/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.3024 - accuracy: 0.9248 - auc: 0.9900\n",
      "Epoch 18: val_loss did not improve from 0.26507\n",
      "218/218 [==============================] - 276s 1s/step - loss: 0.3024 - accuracy: 0.9248 - auc: 0.9900 - val_loss: 0.3782 - val_accuracy: 0.9315 - val_auc: 0.9907\n",
      "Epoch 19/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.3330 - accuracy: 0.9268 - auc: 0.9882\n",
      "Epoch 19: val_loss did not improve from 0.26507\n",
      "218/218 [==============================] - 272s 1s/step - loss: 0.3330 - accuracy: 0.9268 - auc: 0.9882 - val_loss: 0.5969 - val_accuracy: 0.9153 - val_auc: 0.9804\n",
      "Epoch 20/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.3326 - accuracy: 0.9222 - auc: 0.9873\n",
      "Epoch 20: val_loss improved from 0.26507 to 0.20932, saving model to AUG_ensemble.tf\n",
      "INFO:tensorflow:Assets written to: AUG_ensemble.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: AUG_ensemble.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218/218 [==============================] - 413s 2s/step - loss: 0.3326 - accuracy: 0.9222 - auc: 0.9873 - val_loss: 0.2093 - val_accuracy: 0.9496 - val_auc: 0.9959\n",
      "Epoch 21/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.2880 - accuracy: 0.9339 - auc: 0.9898\n",
      "Epoch 21: val_loss did not improve from 0.20932\n",
      "218/218 [==============================] - 271s 1s/step - loss: 0.2880 - accuracy: 0.9339 - auc: 0.9898 - val_loss: 0.3335 - val_accuracy: 0.9264 - val_auc: 0.9867\n",
      "Epoch 22/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.3073 - accuracy: 0.9337 - auc: 0.9891\n",
      "Epoch 22: val_loss did not improve from 0.20932\n",
      "218/218 [==============================] - 276s 1s/step - loss: 0.3073 - accuracy: 0.9337 - auc: 0.9891 - val_loss: 0.5164 - val_accuracy: 0.8992 - val_auc: 0.9728\n",
      "Epoch 23/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.2891 - accuracy: 0.9319 - auc: 0.9896\n",
      "Epoch 23: val_loss did not improve from 0.20932\n",
      "218/218 [==============================] - 272s 1s/step - loss: 0.2891 - accuracy: 0.9319 - auc: 0.9896 - val_loss: 0.3117 - val_accuracy: 0.9335 - val_auc: 0.9909\n",
      "Epoch 24/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.2978 - accuracy: 0.9351 - auc: 0.9906\n",
      "Epoch 24: val_loss did not improve from 0.20932\n",
      "218/218 [==============================] - 272s 1s/step - loss: 0.2978 - accuracy: 0.9351 - auc: 0.9906 - val_loss: 0.3064 - val_accuracy: 0.9375 - val_auc: 0.9867\n",
      "Epoch 25/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.2932 - accuracy: 0.9368 - auc: 0.9910\n",
      "Epoch 25: val_loss did not improve from 0.20932\n",
      "218/218 [==============================] - 272s 1s/step - loss: 0.2932 - accuracy: 0.9368 - auc: 0.9910 - val_loss: 0.4968 - val_accuracy: 0.8881 - val_auc: 0.9747\n",
      "Epoch 26/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.2536 - accuracy: 0.9411 - auc: 0.9922\n",
      "Epoch 26: val_loss did not improve from 0.20932\n",
      "218/218 [==============================] - 273s 1s/step - loss: 0.2536 - accuracy: 0.9411 - auc: 0.9922 - val_loss: 0.6961 - val_accuracy: 0.9062 - val_auc: 0.9672\n",
      "Epoch 27/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.2449 - accuracy: 0.9408 - auc: 0.9920\n",
      "Epoch 27: val_loss did not improve from 0.20932\n",
      "218/218 [==============================] - 272s 1s/step - loss: 0.2449 - accuracy: 0.9408 - auc: 0.9920 - val_loss: 0.6470 - val_accuracy: 0.8690 - val_auc: 0.9631\n",
      "Epoch 28/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.2573 - accuracy: 0.9446 - auc: 0.9915\n",
      "Epoch 28: val_loss did not improve from 0.20932\n",
      "218/218 [==============================] - 271s 1s/step - loss: 0.2573 - accuracy: 0.9446 - auc: 0.9915 - val_loss: 0.2126 - val_accuracy: 0.9476 - val_auc: 0.9948\n",
      "Epoch 29/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.2670 - accuracy: 0.9443 - auc: 0.9912\n",
      "Epoch 29: val_loss did not improve from 0.20932\n",
      "218/218 [==============================] - 272s 1s/step - loss: 0.2670 - accuracy: 0.9443 - auc: 0.9912 - val_loss: 1.3958 - val_accuracy: 0.8438 - val_auc: 0.9416\n",
      "Epoch 30/50\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.2618 - accuracy: 0.9480 - auc: 0.9914\n",
      "Epoch 30: val_loss did not improve from 0.20932\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "218/218 [==============================] - 272s 1s/step - loss: 0.2618 - accuracy: 0.9480 - auc: 0.9914 - val_loss: 0.5143 - val_accuracy: 0.9022 - val_auc: 0.9820\n",
      "Epoch 30: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Concatenate, Dense, Dropout\n",
    "from tensorflow.keras.optimizers.experimental import RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"AUG_ensemble.tf\",\n",
    "                             monitor='val_loss',\n",
    "                             mode='min',\n",
    "                             save_best_only=True, verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0,\n",
    "                          patience=10,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True)\n",
    "\n",
    "epochs = 50\n",
    "callbacks=[checkpoint, earlystop]\n",
    "\n",
    "# Load the base models\n",
    "model1 = load_model('aug_EfficientNetB2.h5')\n",
    "model2 = load_model('aug_ResNet50.h5')\n",
    "model3 = load_model('aug_DenseNet201.h5')\n",
    "model4 = load_model('augnewInceptionV3.h5')\n",
    "\n",
    "# Remove the output layers from the base models\n",
    "model1 = Model(inputs=model1.inputs,\n",
    "               outputs=model1.layers[-2].output,\n",
    "               name='name_of_model_1')\n",
    "model2 = Model(inputs=model2.inputs,\n",
    "               outputs=model2.layers[-2].output,\n",
    "               name='name_of_model_2')\n",
    "model3 = Model(inputs=model3.inputs,\n",
    "               outputs=model3.layers[-2].output,\n",
    "               name='name_of_model_3')\n",
    "model4 = Model(inputs=model4.inputs,\n",
    "               outputs=model4.layers[-2].output,\n",
    "               name='name_of_model_4')\n",
    "\n",
    "# Define input layer for original features\n",
    "input_features = Input(shape=(224, 224, 3), name='input_features')\n",
    "\n",
    "# Generate predictions from each base model\n",
    "pred1 = model1(input_features)\n",
    "pred2 = model2(input_features)\n",
    "pred3 = model3(input_features)\n",
    "pred4 = model4(input_features)\n",
    "\n",
    "# Concatenate the predictions along with the original input features\n",
    "stacked_outputs = Concatenate()([pred1, pred2, pred3, pred4])\n",
    "\n",
    "# Add regularization (L2 regularization) to the output layer\n",
    "stacked_outputs = Dropout(0.5)(stacked_outputs)  # Dropout regularization\n",
    "stacking_output = Dense(num_classes, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.01))(stacked_outputs)\n",
    "\n",
    "# Define the stacking model architecture\n",
    "stacking_model = Model(inputs=input_features, outputs=stacking_output, name='stacking_model')\n",
    "\n",
    "# Compile and train the stacking model\n",
    "stacking_model.compile(loss='categorical_crossentropy',\n",
    "                        optimizer=RMSprop(learning_rate=0.001),\n",
    "                        metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "\n",
    "# Train the stacking model with your training data\n",
    "history = stacking_model.fit(train_generator,\n",
    "                   steps_per_epoch=num_train_samples//batch_size,\n",
    "                   epochs=epochs,\n",
    "                   callbacks=callbacks,\n",
    "                   validation_data=val_generator,\n",
    "                   validation_steps=num_val_samples//batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6fc0373-a54c-49a4-bcd6-9f58467727ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2531994d-0bb8-4cc5-8df7-dabc6fa148cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c501bfc5-3d81-4def-acc3-99131eb9aeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def predict_class(image_path, model, train_generator):\n",
    "    # Load the image\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "\n",
    "    # Convert the image to a numpy array\n",
    "    img_array = img_to_array(img)\n",
    "\n",
    "    # Expand the dimensions of the numpy array\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    # Predict the class of the image\n",
    "    class_prediction = model.predict(img_array)\n",
    "\n",
    "    # Get the predicted class index\n",
    "    class_index = np.argmax(class_prediction)\n",
    "\n",
    "    # Get the class indices and names\n",
    "    class_indices = train_generator.class_indices\n",
    "    class_names = dict((v, k) for k, v in class_indices.items())\n",
    "\n",
    "    # Get the predicted class name\n",
    "    class_name = class_names[class_index]\n",
    "\n",
    "    # Return the predicted class name\n",
    "    return class_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce98d76a-2c6f-43da-87d0-47514b6d360d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bd892e1-8dd4-4ff1-b4ae-8d5c4e288da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 53ms/step\n",
      "dia_ret\n"
     ]
    }
   ],
   "source": [
    "user_input = \"/home/bharatforge/Desktop/new_data_eye/dataset/dia_ret/dia_ret_126.png\"\n",
    "path = user_input\n",
    "class_index = predict_class(path, stacking_model, train_generator)\n",
    "print(class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "226ce715-1d5d-453c-994f-8833ad220095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 56ms/step\n",
      "cataract\n"
     ]
    }
   ],
   "source": [
    "user_input = \"/home/bharatforge/Desktop/new_data_eye/dataset/cataract/cataract_265.jpg\"\n",
    "path = user_input\n",
    "class_index = predict_class(path, stacking_model, train_generator)\n",
    "print(class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f02fdcc8-3d57-4dc5-92db-9bd82be6897a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n",
      "glaucoma\n"
     ]
    }
   ],
   "source": [
    "user_input = \"/home/bharatforge/Desktop/new_data_eye/dataset/glaucoma/glaucoma_02.jpg\"\n",
    "path = user_input\n",
    "class_index = predict_class(path, stacking_model, train_generator)\n",
    "print(class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93fc364c-5079-47cd-9d40-864d6391ec20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "AMD\n"
     ]
    }
   ],
   "source": [
    "user_input = \"/home/bharatforge/Desktop/new_data_eye/dataset/AMD/AMD_232.jpeg\"\n",
    "path = user_input\n",
    "class_index = predict_class(path, stacking_model, train_generator)\n",
    "print(class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0bf66efe-b884-414c-821c-4af0db8ed31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n",
      "normal\n"
     ]
    }
   ],
   "source": [
    "user_input = \"/home/bharatforge/Desktop/new_data_eye/dataset/normal/normal_78.png\"\n",
    "path = user_input\n",
    "class_index = predict_class(path, stacking_model, train_generator)\n",
    "print(class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf10463b-6193-4f6f-854d-67181a94d173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 505 images belonging to 5 classes.\n",
      "32/32 [==============================] - 39s 1s/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMD       0.99      0.95      0.97       102\n",
      "    cataract       1.00      0.98      0.99       102\n",
      "     dia_ret       0.98      0.97      0.98       110\n",
      "    glaucoma       0.93      0.96      0.95       102\n",
      "      normal       0.92      0.97      0.95        89\n",
      "\n",
      "    accuracy                           0.97       505\n",
      "   macro avg       0.97      0.97      0.97       505\n",
      "weighted avg       0.97      0.97      0.97       505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_model = stacking_model\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "# Assuming you have test data in a separate directory\n",
    "test_data_dir = \"/home/bharatforge/Desktop/BE aug new data/data-split/test\"\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_data_dir,\n",
    "                                                  target_size=(img_height, img_width),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False,\n",
    "                                                  class_mode='categorical')\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = loaded_model.predict(test_generator, steps=len(test_generator), verbose=1)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "y_pred_classes = [np.argmax(pred) for pred in y_pred]\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "# Get class labels\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_classes, y_pred_classes, target_names=class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9ad243-574f-4cd3-adfc-1478ec6a760e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5ba20a-7a22-4d85-9d92-0ef21c3cda87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7bea71-f449-4cf0-996f-5bc197e17e27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
