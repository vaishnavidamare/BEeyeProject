{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e156837-d9b4-4f2d-b24f-f58600d6fcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-28 12:20:28.282028: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-28 12:20:28.307998: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-28 12:20:28.308023: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-28 12:20:28.308630: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-28 12:20:28.313016: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-28 12:20:28.811182: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3498 images belonging to 5 classes.\n",
      "Found 997 images belonging to 5 classes.\n",
      "Found 505 images belonging to 5 classes.\n",
      "3498\n",
      "997\n",
      "{'AMD', 'dia_ret', 'normal', 'cataract', 'glaucoma'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "train_data_dir=r\"/home/admin1/Desktop/new data with aug/data-split/train\"\n",
    "val_data_dir=r\"/home/admin1/Desktop/new data with aug/data-split/val\"\n",
    "test_data_dir=r\"/home/admin1/Desktop/new data with aug/data-split/test\"\n",
    "\n",
    "\n",
    "train_datagen=ImageDataGenerator(#rescale=1./255,\n",
    "                                 horizontal_flip=True,\n",
    "                                 )\n",
    "\n",
    "val_datagen=ImageDataGenerator()#rescale=1./255)\n",
    "test_datagen=ImageDataGenerator()#rescale=1./255)\n",
    "\n",
    "img_width,img_height=224,224\n",
    "batch_size=16\n",
    "train_generator=train_datagen.flow_from_directory(train_data_dir,\n",
    "                                                  target_size=(img_height,img_width),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  class_mode='categorical')\n",
    "\n",
    "val_generator=val_datagen.flow_from_directory(val_data_dir,\n",
    "                                              target_size=(img_height,img_width),\n",
    "                                              batch_size=batch_size,\n",
    "                                              class_mode='categorical')\n",
    "\n",
    "test_generator=test_datagen.flow_from_directory(test_data_dir,\n",
    "                                              target_size=(img_height,img_width),\n",
    "                                              batch_size=batch_size,\n",
    "                                              class_mode='categorical')\n",
    "\n",
    "train_class_names = set()\n",
    "num_train_samples=0\n",
    "for i in train_generator.filenames:\n",
    "    train_class_names.add(i.split('/')[0])\n",
    "    num_train_samples+=1\n",
    "print(num_train_samples)\n",
    "train_class_names\n",
    "\n",
    "val_class_names = set()\n",
    "num_val_samples=0\n",
    "for i in val_generator.filenames:\n",
    "    val_class_names.add(i.split('/')[0])\n",
    "    num_val_samples+=1\n",
    "print(num_val_samples)\n",
    "print(val_class_names)\n",
    "\n",
    "num_classes = len(val_class_names)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c79e53-53a5-4a3d-84df-bcb395e1c83e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "225dd5e2-3533-439e-a971-647a31232894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-28 12:20:39.758244: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-02-28 12:20:39.781420: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.layers import GlobalAveragePooling2D,Conv2D, Flatten, BatchNormalization, Dense, Dropout\n",
    "from tensorflow.keras.layers import MaxPooling2D, GlobalAveragePooling2D\n",
    "base_model = ResNet50(\n",
    "                    input_shape=(224, 224, 3),\n",
    "                    weights='imagenet',\n",
    "                    include_top=False)\n",
    "# Freeze the first 10 layers\n",
    "for layer in base_model.layers[:10]:\n",
    "    layer.trainable = False\n",
    "x = base_model.output\n",
    "x = Conv2D(128,(3,3),activation='relu')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.4)(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "model1 = Model(inputs=base_model.inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca7a3c1a-1589-41d8-a16e-33e13a917efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import tensorflow as tf\n",
    "checkpoint = ModelCheckpoint(\"aug_ResNet50.h5\",\n",
    "                             monitor='val_loss',\n",
    "                             mode='min',\n",
    "                             save_best_only=True,verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0,\n",
    "                          patience=7,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True)\n",
    "\n",
    "callbacks=[checkpoint,earlystop]\n",
    "\n",
    "epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac9b14ef-828c-4e18-b61a-fe7a72751857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.4815 - accuracy: 0.8383 - auc: 0.9707\n",
      "Epoch 1: val_loss improved from inf to 0.45041, saving model to aug_ResNet50.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin1/miniconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218/218 [==============================] - 209s 935ms/step - loss: 0.4815 - accuracy: 0.8383 - auc: 0.9707 - val_loss: 0.4504 - val_accuracy: 0.9254 - val_auc: 0.9818\n",
      "Epoch 2/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.2299 - accuracy: 0.9242 - auc: 0.9914\n",
      "Epoch 2: val_loss did not improve from 0.45041\n",
      "218/218 [==============================] - 199s 911ms/step - loss: 0.2299 - accuracy: 0.9242 - auc: 0.9914 - val_loss: 0.4775 - val_accuracy: 0.9294 - val_auc: 0.9793\n",
      "Epoch 3/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.1430 - accuracy: 0.9552 - auc: 0.9950\n",
      "Epoch 3: val_loss improved from 0.45041 to 0.37058, saving model to aug_ResNet50.h5\n",
      "218/218 [==============================] - 199s 913ms/step - loss: 0.1430 - accuracy: 0.9552 - auc: 0.9950 - val_loss: 0.3706 - val_accuracy: 0.9264 - val_auc: 0.9802\n",
      "Epoch 4/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.1363 - accuracy: 0.9661 - auc: 0.9952\n",
      "Epoch 4: val_loss improved from 0.37058 to 0.11062, saving model to aug_ResNet50.h5\n",
      "218/218 [==============================] - 199s 911ms/step - loss: 0.1363 - accuracy: 0.9661 - auc: 0.9952 - val_loss: 0.1106 - val_accuracy: 0.9698 - val_auc: 0.9962\n",
      "Epoch 5/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 0.9759 - auc: 0.9970\n",
      "Epoch 5: val_loss did not improve from 0.11062\n",
      "218/218 [==============================] - 199s 910ms/step - loss: 0.0919 - accuracy: 0.9759 - auc: 0.9970 - val_loss: 0.1536 - val_accuracy: 0.9667 - val_auc: 0.9931\n",
      "Epoch 6/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9793 - auc: 0.9980\n",
      "Epoch 6: val_loss did not improve from 0.11062\n",
      "218/218 [==============================] - 198s 908ms/step - loss: 0.0671 - accuracy: 0.9793 - auc: 0.9980 - val_loss: 0.4237 - val_accuracy: 0.9718 - val_auc: 0.9958\n",
      "Epoch 7/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.0603 - accuracy: 0.9816 - auc: 0.9986\n",
      "Epoch 7: val_loss improved from 0.11062 to 0.08193, saving model to aug_ResNet50.h5\n",
      "218/218 [==============================] - 199s 910ms/step - loss: 0.0603 - accuracy: 0.9816 - auc: 0.9986 - val_loss: 0.0819 - val_accuracy: 0.9768 - val_auc: 0.9972\n",
      "Epoch 8/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.0455 - accuracy: 0.9854 - auc: 0.9988\n",
      "Epoch 8: val_loss did not improve from 0.08193\n",
      "218/218 [==============================] - 199s 912ms/step - loss: 0.0455 - accuracy: 0.9854 - auc: 0.9988 - val_loss: 0.0861 - val_accuracy: 0.9778 - val_auc: 0.9958\n",
      "Epoch 9/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9908 - auc: 0.9990\n",
      "Epoch 9: val_loss did not improve from 0.08193\n",
      "218/218 [==============================] - 199s 911ms/step - loss: 0.0332 - accuracy: 0.9908 - auc: 0.9990 - val_loss: 0.2008 - val_accuracy: 0.9708 - val_auc: 0.9896\n",
      "Epoch 10/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.0368 - accuracy: 0.9905 - auc: 0.9990\n",
      "Epoch 10: val_loss did not improve from 0.08193\n",
      "218/218 [==============================] - 198s 909ms/step - loss: 0.0368 - accuracy: 0.9905 - auc: 0.9990 - val_loss: 0.2726 - val_accuracy: 0.9587 - val_auc: 0.9891\n",
      "Epoch 11/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9908 - auc: 0.9988\n",
      "Epoch 11: val_loss did not improve from 0.08193\n",
      "218/218 [==============================] - 198s 909ms/step - loss: 0.0330 - accuracy: 0.9908 - auc: 0.9988 - val_loss: 0.0948 - val_accuracy: 0.9819 - val_auc: 0.9960\n",
      "Epoch 12/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9931 - auc: 0.9992\n",
      "Epoch 12: val_loss did not improve from 0.08193\n",
      "218/218 [==============================] - 199s 911ms/step - loss: 0.0219 - accuracy: 0.9931 - auc: 0.9992 - val_loss: 0.1573 - val_accuracy: 0.9748 - val_auc: 0.9941\n",
      "Epoch 13/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9908 - auc: 0.9990\n",
      "Epoch 13: val_loss improved from 0.08193 to 0.05046, saving model to aug_ResNet50.h5\n",
      "218/218 [==============================] - 200s 916ms/step - loss: 0.0301 - accuracy: 0.9908 - auc: 0.9990 - val_loss: 0.0505 - val_accuracy: 0.9859 - val_auc: 0.9987\n",
      "Epoch 14/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 0.9911 - auc: 0.9987\n",
      "Epoch 14: val_loss did not improve from 0.05046\n",
      "218/218 [==============================] - 198s 907ms/step - loss: 0.0364 - accuracy: 0.9911 - auc: 0.9987 - val_loss: 0.0701 - val_accuracy: 0.9839 - val_auc: 0.9974\n",
      "Epoch 15/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9937 - auc: 0.9992\n",
      "Epoch 15: val_loss did not improve from 0.05046\n",
      "218/218 [==============================] - 200s 915ms/step - loss: 0.0227 - accuracy: 0.9937 - auc: 0.9992 - val_loss: 0.0791 - val_accuracy: 0.9758 - val_auc: 0.9978\n",
      "Epoch 16/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9945 - auc: 1.0000\n",
      "Epoch 16: val_loss improved from 0.05046 to 0.04754, saving model to aug_ResNet50.h5\n",
      "218/218 [==============================] - 199s 912ms/step - loss: 0.0141 - accuracy: 0.9945 - auc: 1.0000 - val_loss: 0.0475 - val_accuracy: 0.9879 - val_auc: 0.9986\n",
      "Epoch 17/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9951 - auc: 0.9998\n",
      "Epoch 17: val_loss did not improve from 0.04754\n",
      "218/218 [==============================] - 204s 932ms/step - loss: 0.0167 - accuracy: 0.9951 - auc: 0.9998 - val_loss: 0.0512 - val_accuracy: 0.9899 - val_auc: 0.9974\n",
      "Epoch 18/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9968 - auc: 0.9995\n",
      "Epoch 18: val_loss did not improve from 0.04754\n",
      "218/218 [==============================] - 203s 933ms/step - loss: 0.0177 - accuracy: 0.9968 - auc: 0.9995 - val_loss: 0.0675 - val_accuracy: 0.9869 - val_auc: 0.9986\n",
      "Epoch 19/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9957 - auc: 0.9993\n",
      "Epoch 19: val_loss did not improve from 0.04754\n",
      "218/218 [==============================] - 200s 915ms/step - loss: 0.0158 - accuracy: 0.9957 - auc: 0.9993 - val_loss: 0.2394 - val_accuracy: 0.9708 - val_auc: 0.9917\n",
      "Epoch 20/20\n",
      "218/218 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.9980 - auc: 0.9998\n",
      "Epoch 20: val_loss did not improve from 0.04754\n",
      "218/218 [==============================] - 200s 914ms/step - loss: 0.0101 - accuracy: 0.9980 - auc: 0.9998 - val_loss: 0.1698 - val_accuracy: 0.9819 - val_auc: 0.9952\n"
     ]
    }
   ],
   "source": [
    "model1.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=RMSprop(learning_rate=0.0001),\n",
    "                   metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "history1 = model1.fit(train_generator,\n",
    "                         steps_per_epoch=num_train_samples//batch_size,\n",
    "                         epochs=epochs,\n",
    "                         callbacks=callbacks,\n",
    "                         validation_data=val_generator,\n",
    "                         validation_steps=num_val_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96064ffc-772c-4a40-876a-c2cfdedc8eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1c08658-a801-488a-b0c6-b95adfe35fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def predict_class(image_path, model, train_generator):\n",
    "    # Load the image\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "\n",
    "    # Convert the image to a numpy array\n",
    "    img_array = img_to_array(img)\n",
    "\n",
    "    # Expand the dimensions of the numpy array\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    # Predict the class of the image\n",
    "    class_prediction = model.predict(img_array)\n",
    "\n",
    "    # Get the predicted class index\n",
    "    class_index = np.argmax(class_prediction)\n",
    "\n",
    "    # Get the class indices and names\n",
    "    class_indices = train_generator.class_indices\n",
    "    class_names = dict((v, k) for k, v in class_indices.items())\n",
    "\n",
    "    # Get the predicted class name\n",
    "    class_name = class_names[class_index]\n",
    "\n",
    "    # Return the predicted class name\n",
    "    return class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4633daec-6d62-48ff-b4ed-f981ff5656e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the model from the .h5 file\n",
    "loaded_model = tf.keras.models.load_model('aug_ResNet50.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "954472ba-994d-4b24-995b-92c24e13128c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 598ms/step\n",
      "AMD\n"
     ]
    }
   ],
   "source": [
    "user_input = \"/home/admin1/Desktop/BE project/final dataset/AMD/AMD_107.png\"\n",
    "path = user_input\n",
    "class_index = predict_class(path, loaded_model, train_generator)\n",
    "print(class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "927210a4-4301-4929-8676-ddaf44854db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 388ms/step\n",
      "cataract\n"
     ]
    }
   ],
   "source": [
    "user_input = \"/home/admin1/Desktop/BE project/final dataset/cataract/cataract_102.jpg\"\n",
    "path = user_input\n",
    "class_index = predict_class(path, loaded_model, train_generator)\n",
    "print(class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e65974ea-7a77-4d3f-a5cd-3e8f4feffbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 52ms/step\n",
      "dia_ret\n"
     ]
    }
   ],
   "source": [
    "user_input = \"/home/admin1/Desktop/BE project/final dataset/dia_ret/dia_ret_185.png\"\n",
    "path = user_input\n",
    "class_index = predict_class(path, loaded_model, train_generator)\n",
    "print(class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d767fbc1-37fe-4df0-8aad-8b1db906e9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n",
      "glaucoma\n"
     ]
    }
   ],
   "source": [
    "user_input = \"/home/admin1/Desktop/BE project/final dataset/glaucoma/glaucoma_284.png\"\n",
    "path = user_input\n",
    "class_index = predict_class(path, loaded_model, train_generator)\n",
    "print(class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ad8c9ff-ace3-4202-869f-427218c58d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step\n",
      "normal\n"
     ]
    }
   ],
   "source": [
    "user_input = \"/home/admin1/Desktop/BE project/final dataset/normal/normal_159.png\"\n",
    "path = user_input\n",
    "class_index = predict_class(path, loaded_model, train_generator)\n",
    "print(class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6879027-febf-47c8-8bc9-ed508a170e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d31a8bb-9e35-42ae-b8d8-7e9009913e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 505 images belonging to 5 classes.\n",
      "32/32 [==============================] - 23s 596ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         AMD       1.00      1.00      1.00       102\n",
      "    cataract       1.00      0.96      0.98       102\n",
      "     dia_ret       0.96      0.99      0.97       110\n",
      "    glaucoma       0.99      0.99      0.99       102\n",
      "      normal       0.99      0.99      0.99        89\n",
      "\n",
      "    accuracy                           0.99       505\n",
      "   macro avg       0.99      0.99      0.99       505\n",
      "weighted avg       0.99      0.99      0.99       505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "# Assuming you have test data in a separate directory\n",
    "test_data_dir = \"/home/admin1/Desktop/new data with aug/data-split/test\"\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_data_dir,\n",
    "                                                  target_size=(img_height, img_width),\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False,\n",
    "                                                  class_mode='categorical')\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model1.predict(test_generator, steps=len(test_generator), verbose=1)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "y_pred_classes = [np.argmax(pred) for pred in y_pred]\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "# Get class labels\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_classes, y_pred_classes, target_names=class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f86b6e5-ee82-49cd-a844-e72ccc53088b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
